{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9c9773f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Literal\n",
    "from langchain import hub\n",
    "from typing import Literal,List\n",
    "from typing_extensions import TypedDict\n",
    "from langchain.schema import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings,ChatOpenAI\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import Field,BaseModel\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "langchain_api = os.getenv('LANGCHAIN_API_KEY')\n",
    "TAVILY_API_KEY = os.getenv('TAVILY_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32429a59",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14cf5cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "doc_list = [item for sublist in docs for item in sublist ]\n",
    "doc_list\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size = 250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(doc_list)\n",
    "\n",
    "vectorspace = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name='rag_chroma',\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever = vectorspace.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b58c6f",
   "metadata": {},
   "source": [
    "### Retrieval Grader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c86b1607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1900: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n",
      "/var/folders/v9/yz64p8z5579c6d6yx28446qh0000gn/T/ipykernel_36298/402316597.py:21: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  relevant_docs = retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradeDocuments(binary_score='yes')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GradeDocuments(BaseModel):\n",
    "    binary_score : Literal[\"yes\",\"no\"] = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes', or 'no'\"\n",
    "    )\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\" , system),\n",
    "        (\"user\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieved_grader_chain = (grade_prompt|structured_llm_grader)\n",
    "question = \"agent memory\"\n",
    "relevant_docs = retriever.get_relevant_documents(question)\n",
    "retrieved_grader_chain.invoke({'question':question,\"document\":relevant_docs})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ceb180",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c91318f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The agent's memory includes short-term memory for in-context learning and long-term memory for retaining and recalling information over extended periods. Long-term memory is often supported by an external vector store for fast retrieval. Memory is a crucial component in enabling agents to behave based on past experiences and interact with other agents.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "rag_chain = (\n",
    "    prompt|llm|StrOutputParser()\n",
    ")\n",
    "\n",
    "generation = rag_chain.invoke({'question':question,\"context\":relevant_docs})\n",
    "generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5591ea31",
   "metadata": {},
   "source": [
    "### Question Re-writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "917e0fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the role of memory in artificial intelligence agents?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for web search. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\",system),\n",
    "    (\"user\", \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\"),\n",
    "])\n",
    "\n",
    "question_rewriter = (re_write_prompt|llm|StrOutputParser())\n",
    "question_rewriter.invoke({\"question\": question})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07335336",
   "metadata": {},
   "source": [
    "### Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e6101b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_search_tool = TavilySearchResults(k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f6f8bb",
   "metadata": {},
   "source": [
    "### Define Graph State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "323dc056",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(BaseModel):\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search:str\n",
    "    documents: List[str]\n",
    "\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents =retriever.get_relevant_documents(question)\n",
    "\n",
    "    return {\"documents\":documents, \"question\":question}\n",
    "\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state['question']\n",
    "    documents = state['documents']\n",
    "\n",
    "    generation = rag_chain.invoke({'question':question,\"context\":documents})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state['question']\n",
    "    documents = state['documents']\n",
    "\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "\n",
    "    for d in documents:\n",
    "        score = retrieved_grader_chain.invoke({\n",
    "            'question':question,\"document\":documents})\n",
    "        if score == 'yes':\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            web_search='Yes'\n",
    "    return {\"documents\": documents, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "            \n",
    "\n",
    "def transform_query(state):\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state['question']\n",
    "    documents = state['documents']\n",
    "\n",
    "    better_question = question_rewriter({\"question\":question})\n",
    "    return {\"documents\": documents, \"question\": better_question}\n",
    "\n",
    "    \n",
    "def web_search(state):\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state['question']\n",
    "    documents = state['documents']\n",
    "\n",
    "    docs = web_search_tool.invoke({\"question\":question})\n",
    "    docs = web_search_tool.invoke({\"query\":question})\n",
    "    web_content = '\\n'.join([ i['content']  for i in docs])\n",
    "    web_results = Document(page_content=web_content)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"documents\":documents, \"question\":question}\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
    "        )\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cf2cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ebc3dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
