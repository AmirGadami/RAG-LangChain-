{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ac6f903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Literal\n",
    "from pprint import pprint\n",
    "from langchain import hub\n",
    "from typing import Literal,List\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_cohere import CohereEmbeddings, ChatCohere\n",
    "from langgraph.graph import END,START,StateGraph\n",
    "from langchain.schema import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings,ChatOpenAI\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import Field,BaseModel\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "langchain_api = os.getenv('LANGCHAIN_API_KEY')\n",
    "TAVILY_API_KEY = os.getenv('TAVILY_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "COHERE_API_KEY=os.getenv('COHERE_API_KEY')\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc99c3f1",
   "metadata": {},
   "source": [
    "### Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7e0c6892",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "embd = CohereEmbeddings(model=\"embed-multilingual-v3.0\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=512,chunk_overlap=0\n",
    ")\n",
    "splited_text = text_splitter.split_documents(docs_list)\n",
    "\n",
    "vectorspace = Chroma.from_documents(\n",
    "    embedding=embd,\n",
    "    documents=splited_text,\n",
    ")\n",
    "\n",
    "retriever = vectorspace.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133faa25",
   "metadata": {},
   "source": [
    "### Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cde80aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class web_search(BaseModel):\n",
    "    \"\"\"\n",
    "    The internet. Use web_search for questions that are related to anything else than agents, prompt engineering, and adversarial attacks.\n",
    "    \"\"\"\n",
    "    query: str=Field(description=\"The query to use when searching the internet\")\n",
    "\n",
    "class vectorstore(BaseModel):\n",
    "    \"\"\"\n",
    "    A vectorstore containing documents related to agents, prompt engineering, and adversarial attacks. Use the vectorstore for questions on these topics.   \n",
    "    \"\"\"\n",
    "    query: str=Field(description= \"The query to use when searching the vectorstore\")\n",
    "\n",
    "\n",
    "preamble = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
    "Use the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\n",
    "\n",
    "llm = ChatCohere(model=\"command-a-03-2025\",temperature=0)\n",
    "structured_llm_router = llm.bind_tools(tools=[web_search,vectorstore],\n",
    "                                       preamble=preamble)\n",
    "\n",
    "route_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        ('user', \"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_router = route_prompt|structured_llm_router\n",
    "\n",
    "response = question_router.invoke(\n",
    "    {\"question\": \"What are the types of agent memory?\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2bf18d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'web_search'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.additional_kwargs['tool_calls'][0]['function']['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b9ef6e",
   "metadata": {},
   "source": [
    "### Retireval Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dcb96596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1900: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradeDocuments(binary_score='yes')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class GradeDocuments(BaseModel):\n",
    "#     binary_score: Literal[\"yes\",\"no\"] = Field(\n",
    "#         description=\"Documents are relevant to the question: 'yes' or 'no'\"\n",
    "#     )\n",
    "\n",
    "# preamble = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "# If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "# Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "\n",
    "# llm = ChatCohere(model=\"command-a-03-2025\",temperature=0)\n",
    "# structure_llm_grader = llm.with_structured_output(GradeDocuments,preamble=preamble)\n",
    "# grader_prompt = ChatPromptTemplate.from_messages([\n",
    "#     ('human',\"Retrieved Document: \\n\\n{documents} \\n\\n User question: \\n\\n {question}\"),\n",
    "# ])\n",
    "class GradeDocuments(BaseModel):\n",
    "    binary_score : Literal[\"yes\",\"no\"] = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes', or 'no'\"\n",
    "    )\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grader_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\" , system),\n",
    "        (\"user\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader_chain = grader_prompt|structured_llm_grader\n",
    "question = \"What are the types of agent memory?\"\n",
    "docs = retriever.get_relevant_documents(question)[0].page_content\n",
    "retrieval_grader_chain.invoke({\"document\": docs, \"question\": question})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de11c179",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a62f7277",
   "metadata": {},
   "outputs": [],
   "source": [
    "preamble = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\"\"\"\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', preamble),\n",
    "        ('user', \"Question: \\n{question}\\n\\nContext:\\n{document}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "rag_generation = prompt|llm|StrOutputParser()\n",
    "\n",
    "generation= rag_generation.invoke({'question': question, \"document\":docs})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96648e51",
   "metadata": {},
   "source": [
    "### Llm Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2deec0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "preamble = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', preamble),\n",
    "        ('user', \"Question: \\n{question} \\n\\nAnswer:\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "rag_fall_back = prompt|llm|StrOutputParser()\n",
    "generation=rag_fall_back.invoke({\"question\":question})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2058621",
   "metadata": {},
   "source": [
    "### Hallucination Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fd24b45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1900: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradeHallucination(binary_score='yes')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GradeHallucination(BaseModel):\n",
    "    binary_score:Literal [\"yes\",\"no\"] = Field(\n",
    "        description=\"Answer is grounded in facts, 'yes' or 'no\"\n",
    "    )\n",
    "\n",
    "\n",
    "preamble = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
    "Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "structured_llm_hall_grader = llm.with_structured_output(GradeHallucination)\n",
    "\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",preamble),\n",
    "    (\"user\",\"Set of Facts:\\n\\n {documents} \\n\\nLLM Generation:\\n{generation}\")\n",
    "])\n",
    "hellucination_rag = hallucination_prompt|structured_llm_hall_grader\n",
    "hellucination_rag.invoke({'documents':docs,\"generation\":generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5ee884",
   "metadata": {},
   "source": [
    "### Answer Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "73c30a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1900: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradeAnswer(binary_score='yes')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "preamble = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n\n",
    "Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
    "\n",
    "\n",
    "structured_llm_grader_answer = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "    (\"system\",preamble),\n",
    "    (\"user\",\"Question:\\n\\n {question} \\n\\nLLM Generation:\\n{generation}\")\n",
    " \n",
    "    ]\n",
    ")\n",
    "answer_grader_rag = prompt|structured_llm_grader_answer\n",
    "\n",
    "answer_grader_rag.invoke({'question':question,\"generation\":generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c965fbe2",
   "metadata": {},
   "source": [
    "### Web Search Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "13fee61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v9/yz64p8z5579c6d6yx28446qh0000gn/T/ipykernel_19853/263541278.py:1: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  web_search_tool = TavilySearchResults()\n"
     ]
    }
   ],
   "source": [
    "web_search_tool = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cdc6d3",
   "metadata": {},
   "source": [
    "### Building Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5237a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "\n",
    "    question:str\n",
    "    generation:str\n",
    "    documents:List[str]\n",
    "\n",
    "#----------------------------\n",
    "\n",
    "def retrieve(state):\n",
    "    print(\"---RETRIEVE---\")\n",
    "\n",
    "    question = state['question']\n",
    "    documents = retriever.invoke({'question':question})\n",
    "    return {'documents':documents,'question':question}\n",
    "\n",
    "\n",
    "\n",
    "def llm_fallback(state):\n",
    "    print(\"---LLM Fallback---\")\n",
    "\n",
    "    generation = rag_fall_back.invoke({\"question\": question})\n",
    "    return {\"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    print(\"---GENERATE---\")\n",
    "\n",
    "    question = state['question']\n",
    "    documents = state['documents']\n",
    "    if not isinstance(documents, list):\n",
    "        documents = [documents]\n",
    "    generation = rag_generation.invoke({\"documents\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\":generation}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "\n",
    "    question = state['question']\n",
    "    documents = state['documents']\n",
    "\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader_chain.invoke({\"document\": docs, \n",
    "                                               \"question\": question}).binary_score\n",
    "        if score=='yes':\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    print(\"---WEB SEARCH---\")\n",
    "\n",
    "    question=state['question']\n",
    "    docs = web_search_tool.invoke({\"quesry\":question})\n",
    "    web_search = \"\\n\".join(d['content'] for d in docs)\n",
    "    web_result = Document(page_content=web_search)\n",
    "    return{\"documents\":web_result,\"question\":question}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f518f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = web_search_tool.invoke(dcontent{\"query\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bd86faa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'What Is AI Agent Memory? Types, Tradeoffs and Implementation',\n",
       "  'url': 'https://www.techtarget.com/searchenterpriseai/tip/What-is-AI-agent-memory-Types-tradeoffs-and-implementation',\n",
       "  'content': '## Types of AI agent memory\\n\\nAI agents can use two broad types of memory: short-term memory and long-term memory. Each type of memory can be designed in different common variations.\\n\\n### Short-term memory [...] Memory type: STM vs. LTM. STM is used to maintain context and continuity during a single session or task. LTM retains information across sessions and tasks, so an AI agent can learn, adapt and glean insights. LTM types include episodic, semantic and procedural, each providing unique benefits to an AI agent. STM uses relatively little memory capacity and can offer fast performance, while LTM can use extensive memory capacity, which introduces latency and inefficiency to the storage and [...] LTM is often implemented with well-established technologies, like databases and knowledge graphs, and can include vector embeddings. LTM also relies heavily on RAG techniques that enable the AI agent to locate required information from a stored knowledge resource to generate improved responses.\\n\\nThere are typically three types of long-term AI agent memory, including the following:',\n",
       "  'score': 0.89567816},\n",
       " {'title': 'Memory Types in Agentic AI: A Breakdown | by Gokcer Belgusen',\n",
       "  'url': 'https://medium.com/@gokcerbelgusen/memory-types-in-agentic-ai-a-breakdown-523c980921ec',\n",
       "  'content': '# How These Work Together in Agentic AI\\n\\nIn an agentic AI system, these memory types collaborate to create a capable, goal-driven agent. Short-term memory handles immediate demands, while long-term memory — encompassing semantic, episodic, and procedural elements — builds a deeper foundation.\\n\\nSemantic memory provides the facts, episodic memory offers lessons from experience, and procedural memory ensures smooth execution. [...] Sitemap\\n\\nOpen in app\\n\\nSign in\\n\\nSign in\\n\\n# Memory Types in Agentic AI: A Breakdown\\n\\nGokcer Belgusen\\n\\n4 min readApr 6, 2025\\n\\nAgentic AI — systems designed to act autonomously, make decisions, and pursue goals — relies on various types of memory to function effectively. Drawing from cognitive science concepts, these memory types include\\n\\n semantic\\n episodic\\n short-term\\n procedural and\\n long-term memory. [...] Episodic memory is the AI’s record of specific experiences or events, tied to a time and context. Think of it as the agent’s personal history — like recalling, “Last Tuesday, I helped a user debug code and got stuck on a syntax error.” In agentic AI, episodic memory allows the system to reflect on past interactions or actions, learning from successes or mistakes. This type of memory adds a narrative layer, helping the AI adjust its behavior based on what it has directly encountered.',\n",
       "  'score': 0.85186684},\n",
       " {'title': 'What are AI agents? Definition, examples, and types | Google Cloud',\n",
       "  'url': 'https://cloud.google.com/discover/what-are-ai-agents',\n",
       "  'content': 'Memory: The agent is equipped in general with short term, long term, consensus, and episodic memory.Short term memory for immediate interactions, long-term memory for historical data and conversations, episodic memory for past interactions, and consensus memory for shared information among agents. The agent can maintain context, learn from experiences, and improve performance by recalling past interactions and adapting to new situations.',\n",
       "  'score': 0.8342047},\n",
       " {'title': 'Memory in multi-agent systems: technical implementations - Medium',\n",
       "  'url': 'https://medium.com/@cauri/memory-in-multi-agent-systems-technical-implementations-770494c0eca7',\n",
       "  'content': '1. Immediate Working Memory — Information that must remain constantly accessible, similar to how you don’t need to recall how to speak or walk consciously\\n2. Searchable Episodic Memory — Information the agent must actively retrieve, comparable to how you search your mind for a specific conversation or event\\n3. Procedural Memory — Skills and learned behaviours that become automatic, like your ability to type without thinking about individual keys [...] Modern agent systems require more nuanced memory types that mirror human cognitive categories:',\n",
       "  'score': 0.7732339},\n",
       " {'title': 'What Is AI Agent Memory? | IBM',\n",
       "  'url': 'https://www.ibm.com/think/topics/ai-agent-memory',\n",
       "  'content': \"Researchers categorize agentic memory in much the same way that psychologists categorize human memory. The influential Cognitive Architectures for Language Agents (CoALA) paper1 from a team at Princeton University describes different types of memory as:\\n\\n### Short-term memory [...] Episodic memory allows AI agents to recall specific past experiences, similar to how humans remember individual events. This type of memory is useful for case-based reasoning, where an AI learns from past events to make better decisions in the future.Episodic memory is often implemented by logging key events, actions and their outcomes in a structured format that the agent can access when making decisions.For example, an AI-powered financial advisor might remember a user's past investment\",\n",
       "  'score': 0.7732339}]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd0418f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
