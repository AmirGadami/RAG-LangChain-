{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ac6f903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Literal\n",
    "from pprint import pprint\n",
    "from langchain import hub\n",
    "from typing import Literal,List\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_cohere import CohereEmbeddings, ChatCohere\n",
    "from langgraph.graph import END,START,StateGraph\n",
    "from langchain.schema import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings,ChatOpenAI\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import Field,BaseModel\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "langchain_api = os.getenv('LANGCHAIN_API_KEY')\n",
    "TAVILY_API_KEY = os.getenv('TAVILY_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "COHERE_API_KEY=os.getenv('COHERE_API_KEY')\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc99c3f1",
   "metadata": {},
   "source": [
    "### Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7e0c6892",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "embd = CohereEmbeddings(model=\"embed-multilingual-v3.0\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=512,chunk_overlap=0\n",
    ")\n",
    "splited_text = text_splitter.split_documents(docs_list)\n",
    "\n",
    "vectorspace = Chroma.from_documents(\n",
    "    embedding=embd,\n",
    "    documents=splited_text,\n",
    ")\n",
    "\n",
    "retriever = vectorspace.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133faa25",
   "metadata": {},
   "source": [
    "### Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cde80aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class web_search(BaseModel):\n",
    "    \"\"\"\n",
    "    The internet. Use web_search for questions that are related to anything else than agents, prompt engineering, and adversarial attacks.\n",
    "    \"\"\"\n",
    "    query: str=Field(description=\"The query to use when searching the internet\")\n",
    "\n",
    "class vectorstore(BaseModel):\n",
    "    \"\"\"\n",
    "    A vectorstore containing documents related to agents, prompt engineering, and adversarial attacks. Use the vectorstore for questions on these topics.   \n",
    "    \"\"\"\n",
    "    query: str=Field(description= \"The query to use when searching the vectorstore\")\n",
    "\n",
    "\n",
    "preamble = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
    "Use the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\n",
    "\n",
    "llm = ChatCohere(model=\"command-a-03-2025\",temperature=0)\n",
    "structured_llm_router = llm.bind_tools(tools=[web_search,vectorstore],\n",
    "                                       preamble=preamble)\n",
    "\n",
    "route_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        ('user', \"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_router = route_prompt|structured_llm_router\n",
    "\n",
    "response = question_router.invoke(\n",
    "    {\"question\": \"How are you?\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b9ef6e",
   "metadata": {},
   "source": [
    "### Retireval Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dcb96596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1900: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradeDocuments(binary_score='yes')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class GradeDocuments(BaseModel):\n",
    "#     binary_score: Literal[\"yes\",\"no\"] = Field(\n",
    "#         description=\"Documents are relevant to the question: 'yes' or 'no'\"\n",
    "#     )\n",
    "\n",
    "# preamble = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "# If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "# Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "\n",
    "# llm = ChatCohere(model=\"command-a-03-2025\",temperature=0)\n",
    "# structure_llm_grader = llm.with_structured_output(GradeDocuments,preamble=preamble)\n",
    "# grader_prompt = ChatPromptTemplate.from_messages([\n",
    "#     ('human',\"Retrieved Document: \\n\\n{documents} \\n\\n User question: \\n\\n {question}\"),\n",
    "# ])\n",
    "class GradeDocuments(BaseModel):\n",
    "    binary_score : Literal[\"yes\",\"no\"] = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes', or 'no'\"\n",
    "    )\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grader_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\" , system),\n",
    "        (\"user\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader_chain = grader_prompt|structured_llm_grader\n",
    "question = \"What are the types of agent memory?\"\n",
    "docs = retriever.get_relevant_documents(question)[0].page_content\n",
    "retrieval_grader_chain.invoke({\"document\": docs, \"question\": question})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de11c179",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a62f7277",
   "metadata": {},
   "outputs": [],
   "source": [
    "preamble = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\"\"\"\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', preamble),\n",
    "        ('user', \"Question: \\n{question}\\n\\nContext:\\n{document}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "rag_generation = prompt|llm|StrOutputParser()\n",
    "\n",
    "generation= rag_generation.invoke({'question': question, \"document\":docs})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96648e51",
   "metadata": {},
   "source": [
    "### Llm Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2deec0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "preamble = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', preamble),\n",
    "        ('user', \"Question: \\n{question} \\n\\nAnswer:\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "rag_fall_back = prompt|llm|StrOutputParser()\n",
    "generation=rag_fall_back.invoke({\"question\":question})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2058621",
   "metadata": {},
   "source": [
    "### Hallucination Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fd24b45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1900: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradeHallucination(binary_score='yes')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GradeHallucination(BaseModel):\n",
    "    binary_score:Literal [\"yes\",\"no\"] = Field(\n",
    "        description=\"Answer is grounded in facts, 'yes' or 'no\"\n",
    "    )\n",
    "\n",
    "\n",
    "preamble = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
    "Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "structured_llm_hall_grader = llm.with_structured_output(GradeHallucination)\n",
    "\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",preamble),\n",
    "    (\"user\",\"Set of Facts:\\n\\n {documents} \\n\\nLLM Generation:\\n{generation}\")\n",
    "])\n",
    "hellucination_rag = hallucination_prompt|structured_llm_hall_grader\n",
    "hellucination_rag.invoke({'documents':docs,\"generation\":generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5ee884",
   "metadata": {},
   "source": [
    "### Answer Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "73c30a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1900: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradeAnswer(binary_score='yes')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "preamble = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n\n",
    "Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
    "\n",
    "\n",
    "structured_llm_grader_answer = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "    (\"system\",preamble),\n",
    "    (\"user\",\"Question:\\n\\n {question} \\n\\nLLM Generation:\\n{generation}\")\n",
    " \n",
    "    ]\n",
    ")\n",
    "answer_grader_rag = prompt|structured_llm_grader_answer\n",
    "\n",
    "answer_grader_rag.invoke({'question':question,\"generation\":generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c965fbe2",
   "metadata": {},
   "source": [
    "### Web Search Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "13fee61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v9/yz64p8z5579c6d6yx28446qh0000gn/T/ipykernel_19853/263541278.py:1: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  web_search_tool = TavilySearchResults()\n"
     ]
    }
   ],
   "source": [
    "web_search_tool = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cdc6d3",
   "metadata": {},
   "source": [
    "### Building Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5237a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "\n",
    "    question:str\n",
    "    generation:str\n",
    "    documents:List[str]\n",
    "\n",
    "#----------------------------\n",
    "\n",
    "def retrieve(state):\n",
    "    print(\"---RETRIEVE---\")\n",
    "\n",
    "    question = state['question']\n",
    "    documents = retriever.invoke({'question':question})\n",
    "    return {'documents':documents,'question':question}\n",
    "\n",
    "\n",
    "\n",
    "def llm_fallback(state):\n",
    "    print(\"---LLM Fallback---\")\n",
    "\n",
    "    generation = rag_fall_back.invoke({\"question\": question})\n",
    "    return {\"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    print(\"---GENERATE---\")\n",
    "\n",
    "    question = state['question']\n",
    "    documents = state['documents']\n",
    "    if not isinstance(documents, list):\n",
    "        documents = [documents]\n",
    "    generation = rag_generation.invoke({\"documents\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\":generation}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "\n",
    "    question = state['question']\n",
    "    documents = state['documents']\n",
    "\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader_chain.invoke({\"document\": docs, \n",
    "                                               \"question\": question}).binary_score\n",
    "        if score=='yes':\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    print(\"---WEB SEARCH---\")\n",
    "\n",
    "    question=state['question']\n",
    "    docs = web_search_tool.invoke({\"quesry\":question})\n",
    "    web_search = \"\\n\".join(d['content'] for d in docs)\n",
    "    web_result = Document(page_content=web_search)\n",
    "    return{\"documents\":web_result,\"question\":question}\n",
    "\n",
    "\n",
    "def route_question(state):\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    source = question_router({\"question\":question})\n",
    "\n",
    "    if \"tool_calls\" not in source.additional_kwargs:\n",
    "        print(\"---ROUTE QUESTION TO LLM---\")\n",
    "        return llm_fallback\n",
    "    if len(source.additional_kwargs[\"tool_calls\"]) == 0:\n",
    "        raise \"Router could not decide source\"\n",
    "\n",
    "    datasource = source.additional_kwargs['tool_calls'][0]['function']['name'] \n",
    "    if datasource == \"vectorspace\":\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\"\n",
    "    elif datasource == \"web_search\":\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        print(\"---ROUTE QUESTION TO LLM---\")\n",
    "        return \"llm_fallback\"\n",
    "    \n",
    "\n",
    "    \n",
    "def decide_to_generate(state):\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "\n",
    "    state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, WEB SEARCH---\")\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "    return \"generate\"\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fcd0418f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tool_calls'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[122]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43madditional_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtool_calls\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mfunction\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mKeyError\u001b[39m: 'tool_calls'"
     ]
    }
   ],
   "source": [
    "response.additional_kwargs['tool_calls'][0]['function']['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d30045",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
