{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "07224099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bs4\n",
    "import numpy as np\n",
    "from typing import Literal\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough,RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "import tiktoken\n",
    "from langchain.load import loads, dumps\n",
    "from operator import itemgetter\n",
    "load_dotenv()\n",
    "langchain_api = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "65029e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(parse_only =bs4.SoupStrainer( class_=(\"post-content\", \"post-title\", \"post-header\" )))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f53f226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f71841bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b736680c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas'])\n",
      "ID: 910181a3-dee0-44c8-af8f-d3b934ecc9bb\n",
      "Text: LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool con\n",
      "Meta: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "==================================================\n",
      "ID: dc146721-f5f7-418e-82d7-83f8601ea8ff\n",
      "Text: Memory\n",
      "\n",
      "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
      "Long-term memory: This provides the agent with th\n",
      "Meta: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "==================================================\n",
      "ID: b767d5e0-ca36-42b0-9fe5-535ba6928c0e\n",
      "Text: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a s\n",
      "Meta: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "==================================================\n",
      "ID: 3ca9502c-2cbc-4d1b-894f-b8bbc8059499\n",
      "Text: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outl\n",
      "Meta: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "==================================================\n",
      "ID: 8adf90ce-36f7-41a2-a2a8-5bf9d166bdda\n",
      "Text: Self-Reflection#\n",
      "Self-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in r\n",
      "Meta: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "out = vectorstore._collection.get(include=[\"documents\", \"metadatas\"])\n",
    "print(out.keys())\n",
    "\n",
    "for i in range(min(5,len(out['ids']))):\n",
    "    print(\"ID:\", out[\"ids\"][i])\n",
    "    print(\"Text:\", out[\"documents\"][i][:200])\n",
    "    print(\"Meta:\", out[\"metadatas\"][i])\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d96de6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore._collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6e4d1167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Use the context to answer the question.\\n\\nContext:\\n{context}\\n\\nQuestion: {question}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Use the context to answer the question.\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\"\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1c586aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps, making them more manageable. This process can be done through prompting techniques like Chain of Thought (CoT) or Tree of Thoughts, task-specific instructions, or with the help of external classical planners like in the LLM+P approach.'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name = 'gpt-3.5-turbo',temperature=0)\n",
    "def format_docs(docs):\n",
    "    return '\\n\\n'.join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e75c9966",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Do you think Amir is going to become very rich by the age 40 years old?\"\n",
    "document = 'Amir will is going to gain networth of 200 million by the time he is 40 years ols. His income will get to at least 1 million per month.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4b7a5a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "len(encoding.encode(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fe3f3dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embd = OpenAIEmbeddings()\n",
    "query_result = embd.embed_query(question) \n",
    "document_result = embd.embed_documents([document])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "09b17c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(vec1,vec2):\n",
    "    dot_product = np.dot(vec1,vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "63e59883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9074268061127665)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(query_result,document_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7d507ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\" Answer the question based on the following contex:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt)\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "20e1d8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition in a LLM-powered autonomous agent system involves breaking down large tasks into smaller, more manageable subgoals. This process enables the agent to efficiently handle complex tasks by dividing them into simpler steps. Task decomposition can be achieved through various methods, such as using simple prompting for LLM, task-specific instructions, or human inputs. By breaking down tasks into smaller components, the agent can effectively plan and execute its actions to achieve the overall goal.'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm | StrOutputParser()\n",
    "chain.invoke({'context':docs, 'question':'What is Task Decomposition'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "65b27a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Task decomposition is a technique used by agents to break down complex tasks into smaller and simpler steps. This process involves transforming big tasks into multiple manageable tasks, allowing the agent to plan ahead and interpret the model's thinking process. Techniques such as Chain of Thought (CoT) and Tree of Thoughts extend task decomposition by exploring multiple reasoning possibilities at each step and generating multiple thoughts per step, creating a tree structure.\""
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain = (\n",
    "    {'context': retriever, 'question': RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke('What is Task Decomposition')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36efea2e",
   "metadata": {},
   "source": [
    "#### Multi Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "110104ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "generate_queries = (\n",
    "      prompt\n",
    "    | ChatOpenAI(model='gpt-3.5-turbo',temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x:x.split('\\n'))\n",
    ")\n",
    "\n",
    "def get_unique_union(documents):\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "    \n",
    "\n",
    "# Testing\n",
    "\n",
    "# queries = generate_queries.invoke(\"what is the Decomposition in LLM\")\n",
    "# batches = [retriever.get_relevant_documents(q) for q in queries]\n",
    "# unique_docs = get_unique_union(batches)\n",
    "\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retriever_chain = generate_queries| retriever.map() | get_unique_union\n",
    "docs = retriever_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "65fdd89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents involves breaking down complex tasks into smaller and simpler steps using techniques like Chain of Thought (CoT) and Tree of Thoughts. Additionally, LLM agents can also rely on external classical planners for long-horizon planning using the Planning Domain Definition Language (PDDL) as an intermediate interface.'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "final_rag_chain = ( {'context': retriever_chain,\n",
    "                     'question': itemgetter('question')}\n",
    "                   |prompt\n",
    "                   |llm|StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({'question':question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a549d1",
   "metadata": {},
   "source": [
    "#### RAG Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "cf8d5dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents involves breaking down complex tasks into smaller and simpler steps using techniques such as prompting, task-specific instructions, and human inputs. Additionally, a distinct approach known as LLM+P involves outsourcing the planning step to an external classical planner that utilizes the Planning Domain Definition Language (PDDL) to describe the planning problem and generate a plan that is then translated back into natural language. This allows LLM agents to effectively plan and execute tasks by leveraging external tools and domain-specific knowledge.'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion\n",
    "    |ChatOpenAI(model = 'gpt-3.5-turbo',temperature=0)\n",
    "    |StrOutputParser()\n",
    "    |(lambda x: x.split('\\n'))\n",
    "    \n",
    ")\n",
    "\n",
    "# Testing\n",
    "# results = generate_queries.invoke({'question':\"what is decomposition in LLM\"})\n",
    "\n",
    "def reciprocal_rank_fusion(results:list[list],k=60):\n",
    "\n",
    "    fused_score ={}\n",
    "\n",
    "    for docs in results:\n",
    "        for rank,doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            fused_score[doc_str] = fused_score.get(doc_str,0)+1 /(rank+k)\n",
    "\n",
    "    reranked_items =[(loads(doc),score) for doc ,score in sorted(fused_score.items(), key=lambda x:x[1],reverse=True)]\n",
    "    return reranked_items\n",
    "\n",
    "retrieval_chain = (generate_queries\n",
    "                   |retriever.map()\n",
    "                   |reciprocal_rank_fusion\n",
    ")\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "docs = retrieval_chain.invoke({'question':question})\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question:{question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "fusion_chain = ( {'context':retrieval_chain, 'question':RunnablePassthrough()}\n",
    "                |prompt\n",
    "                |llm\n",
    "                |StrOutputParser()\n",
    "\n",
    ")\n",
    "fusion_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb472bc",
   "metadata": {},
   "source": [
    "#### Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ac7af1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposition\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries_decomposition =(\n",
    "    prompt_decomposition\n",
    "    |llm\n",
    "    |StrOutputParser()\n",
    "    |(lambda x:x.split('\\n'))\n",
    ")\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions = generate_queries_decomposition.invoke(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11887e21",
   "metadata": {},
   "source": [
    "Answer recursively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4fe62571",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "here is the question you need to answer:\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "31fed5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "\n",
    "for q in questions:\n",
    "    rag_chain =({\n",
    "        'context': itemgetter('question') |retriever,\n",
    "        'question': itemgetter('question'),\n",
    "        'q_a_pairs': itemgetter('q_a_pairs')}\n",
    "        |decomposition_prompt\n",
    "        |llm\n",
    "        |StrOutputParser()\n",
    "    )\n",
    "    answer = rag_chain.invoke({'question':q,'q_a_pairs':q_a_pairs})\n",
    "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q,answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1976a34",
   "metadata": {},
   "source": [
    "Answer individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8eb464be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main components of an LLM-powered autonomous agent system include the LLM as the core controller, planning for task breakdown, subgoal decomposition for task management, reflection for self-criticism and learning, refinement for problem-solving enhancement, and memory for information storage and retrieval. These components work together to enable the autonomous agent system to function effectively, solve complex problems, and achieve autonomy in decision-making and task execution.'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"\"\"\n",
    "\n",
    "prompt_rag = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def retrieve_rag_chain(question,prompt,chain):\n",
    "    sub_questions = chain.invoke(question)\n",
    "\n",
    "    rag_results =[]\n",
    "\n",
    "    for q in sub_questions:\n",
    "        retrieved = retriever.get_relevant_documents(q)\n",
    "        answer =(prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved, \n",
    "                                                                \"question\": q})\n",
    "        rag_results.append(answer)\n",
    "        \n",
    "    return questions,sub_questions\n",
    "questions,answers = retrieve_rag_chain(question,prompt_rag,generate_queries_decomposition)\n",
    "\n",
    "\n",
    "def format_qa_paris(questions, answers):\n",
    "\n",
    "    fromat_strings=''\n",
    "\n",
    "    for i, (questions,answers) in enumerate(zip(questions,answers),start=1):\n",
    "         fromat_strings += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return fromat_strings.strip()  \n",
    "\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "\n",
    "context = format_qa_paris(questions,answers)\n",
    "\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "final_rag_chain=(\n",
    "    prompt\n",
    "    |llm\n",
    "    |StrOutputParser()\n",
    ").invoke({'context':context, 'question':question})\n",
    "final_rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e1b38c",
   "metadata": {},
   "source": [
    "#### Step Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7997b2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\",\"{input}\"),\n",
    "    (\"ai\",\"{output}\")\n",
    "])\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "     \"system\", \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",   \n",
    "    ),\n",
    "    few_shot_prompt,\n",
    "    (\"user\",\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# # Testing\n",
    "generate_queries_step_back = (prompt|llm|StrOutputParser())\n",
    "# generate_queries_step_back\n",
    "\n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "chain = (\n",
    "    {\"normal_context\": itemgetter('question')|retriever,\n",
    "          \"step_back_context\":generate_queries_step_back |retriever,\n",
    "          'question': itemgetter('question')}\n",
    "\n",
    "          |response_prompt\n",
    "          |llm|StrOutputParser()\n",
    "\n",
    "\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "360d62e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition in LLM refers to the process of breaking down complex tasks into smaller, more manageable subgoals. This allows the LLM (large language model) to efficiently handle intricate tasks by dividing them into smaller steps. There are several ways in which task decomposition can be achieved within the context of LLM:\n",
      "\n",
      "1. **LLM with Simple Prompting**: One approach to task decomposition involves using simple prompts to guide the LLM in breaking down tasks. For example, prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" can help the LLM organize the task into smaller components.\n",
      "\n",
      "2. **Task-Specific Instructions**: Another method is to provide task-specific instructions to the LLM. For instance, instructing the LLM to \"Write a story outline\" when the task involves creating a novel can help in task decomposition tailored to the specific requirements of the task.\n",
      "\n",
      "3. **Human Inputs**: Task decomposition can also be achieved through human inputs, where individuals provide guidance or input to the LLM on how to break down the task effectively.\n",
      "\n",
      "In addition to these methods, there is a distinct approach known as LLM+P, as proposed by Liu et al. in 2023. This approach involves leveraging an external classical planner for long-horizon planning. The process includes translating the problem into a Planning Domain Definition Language (PDDL), generating a PDDL plan using a classical planner, and then translating the plan back into natural language. This outsourcing of the planning step to an external tool assumes the availability of domain-specific PDDL and a suitable planner, which is common in certain robotic setups.\n",
      "\n",
      "Overall, task decomposition in LLM plays a crucial role in enabling the model to effectively tackle complex tasks by breaking them down into smaller, more manageable subgoals.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({'question': \"what is decomposition in LLM\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c40b36",
   "metadata": {},
   "source": [
    "#### HyDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "3f7287a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition for LLM agents can be done by using simple prompting like \"Steps for XYZ\", \"What are the subgoals for achieving XYZ?\", task-specific instructions, or with human inputs. Additionally, there is a distinct approach called LLM+P that involves relying on an external classical planner to do long-horizon planning using the Planning Domain Definition Language (PDDL) as an intermediate interface.\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde | ChatOpenAI(temperature=0) | StrOutputParser() \n",
    ")\n",
    "\n",
    "# Run\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_docs_for_retrieval.invoke({\"question\":question})\n",
    "\n",
    "retrieval_chain = generate_docs_for_retrieval | retriever \n",
    "retrieved_docs = retrieval_chain.invoke({\"question\":question})\n",
    "\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(final_rag_chain.invoke({\"context\":retrieved_docs,\"question\":question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5549375a",
   "metadata": {},
   "source": [
    "#### Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "3a9b0bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1900: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1913: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo-0125 since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RouteQuery(datasource='python_docs')"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RouteQuery(BaseModel):\n",
    "    datasource : Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user Question choose which datasource would be the most relevant for answering their question\"\n",
    "    )\n",
    "\n",
    "\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "system_prompt =  \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
    "\n",
    "Based on the programming language the question is referring to, route it to the relevant data source.\"\"\"\n",
    "\n",
    "question = \"\"\"Why doesn't the following code work:\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
    "prompt.invoke(\"french\")\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system',system_prompt),\n",
    "    ('user',\"{question}\")\n",
    "])\n",
    "\n",
    "router = (prompt | structured_llm)\n",
    "\n",
    "\n",
    "result = router.invoke({'question':question})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd068a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
