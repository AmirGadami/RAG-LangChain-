{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "07224099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bs4\n",
    "import numpy as np\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import tiktoken\n",
    "from langchain.load import loads, dumps\n",
    "from operator import itemgetter\n",
    "load_dotenv()\n",
    "langchain_api = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "65029e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(parse_only =bs4.SoupStrainer( class_=(\"post-content\", \"post-title\", \"post-header\" )))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f53f226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f71841bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b736680c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas'])\n",
      "ID: beef1b17-0aae-4402-849d-f624debc1eee\n",
      "Text: LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool con\n",
      "Meta: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "==================================================\n",
      "ID: 33227605-ec9c-4783-b6f9-8a6f7199f8d5\n",
      "Text: Memory\n",
      "\n",
      "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
      "Long-term memory: This provides the agent with th\n",
      "Meta: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "==================================================\n",
      "ID: b79bf339-9e4e-452a-aa5a-49c286d7c7a9\n",
      "Text: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a s\n",
      "Meta: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "==================================================\n",
      "ID: b45340cc-017f-4f10-98de-2c1031091207\n",
      "Text: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outl\n",
      "Meta: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "==================================================\n",
      "ID: e4f944c6-42d4-4886-a32e-6a1e1f32d86c\n",
      "Text: Self-Reflection#\n",
      "Self-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in r\n",
      "Meta: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "out = vectorstore._collection.get(include=[\"documents\", \"metadatas\"])\n",
    "print(out.keys())\n",
    "\n",
    "for i in range(min(5,len(out['ids']))):\n",
    "    print(\"ID:\", out[\"ids\"][i])\n",
    "    print(\"Text:\", out[\"documents\"][i][:200])\n",
    "    print(\"Meta:\", out[\"metadatas\"][i])\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d96de6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore._collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6e4d1167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Use the context to answer the question.\\n\\nContext:\\n{context}\\n\\nQuestion: {question}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Use the context to answer the question.\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\"\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1c586aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is the process of breaking down a complex task into smaller and simpler steps to make it more manageable and easier to accomplish. This can be done through techniques like Chain of Thought (CoT) or Tree of Thoughts, which help in organizing and structuring the steps needed to complete a task.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name = 'gpt-3.5-turbo',temperature=0)\n",
    "def format_docs(docs):\n",
    "    return '\\n\\n'.join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e75c9966",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Do you think Amir is going to become very rich by the age 40 years old?\"\n",
    "document = 'Amir will is going to gain networth of 200 million by the time he is 40 years ols. His income will get to at least 1 million per month.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4b7a5a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "len(encoding.encode(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3f3dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embd = OpenAIEmbeddings()\n",
    "query_result = embd.embed_query(question) \n",
    "document_result = embd.embed_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "09b17c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(vec1,vec2):\n",
    "    dot_product = np.dot(vec1,vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "63e59883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9074268061127665)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(query_result,document_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7d507ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\" Answer the question based on the following contex:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt)\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "20e1d8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition in the context of LLM-powered autonomous agents refers to breaking down large tasks into smaller, more manageable subgoals. This process enables the agent to efficiently handle complex tasks by dividing them into simpler steps. Task decomposition can be achieved through various methods, such as using simple prompting for LLM, task-specific instructions, or human inputs. Additionally, techniques like Chain of Thought and Tree of Thoughts are used to enhance model performance on complex tasks by decomposing them into multiple manageable steps.'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm | StrOutputParser()\n",
    "chain.invoke({'context':docs, 'question':'What is Task Decomposition'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "65b27a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Task decomposition is a technique used by agents to break down complex tasks into smaller and simpler steps. This process involves transforming big tasks into multiple manageable tasks, allowing the agent to plan ahead and interpret the model's thinking process. Techniques like Chain of Thought (CoT) and Tree of Thoughts extend task decomposition by exploring multiple reasoning possibilities at each step and generating multiple thoughts per step, creating a tree structure.\""
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain = (\n",
    "    {'context': retriever, 'question': RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke('What is Task Decomposition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f2a142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36efea2e",
   "metadata": {},
   "source": [
    "Multi Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "110104ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "generate_queries = (\n",
    "      prompt\n",
    "    | ChatOpenAI(model='gpt-3.5-turbo',temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x:x.split('\\n'))\n",
    ")\n",
    "\n",
    "def get_unique_union(documents):\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "    \n",
    "\n",
    "# Testing\n",
    "\n",
    "# queries = generate_queries.invoke(\"what is the Decomposition in LLM\")\n",
    "# batches = [retriever.get_relevant_documents(q) for q in queries]\n",
    "# unique_docs = get_unique_union(batches)\n",
    "\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retriever_chain = generate_queries| retriever.map() | get_unique_union\n",
    "docs = retriever_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "65fdd89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents involves breaking down complex tasks into smaller and simpler steps using techniques such as Chain of Thought (CoT) or Tree of Thoughts. It can be done through simple prompting, task-specific instructions, or with the help of an external classical planner in the LLM+P approach.'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "final_rag_chain = ( {'context': retriever_chain,\n",
    "                     'question': itemgetter('question')}\n",
    "                   |prompt\n",
    "                   |llm|StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({'question':question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8d5dea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
