{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "07224099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bs4\n",
    "import numpy as np\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import tiktoken\n",
    "from langchain.load import loads, dumps\n",
    "from operator import itemgetter\n",
    "load_dotenv()\n",
    "langchain_api = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "65029e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(parse_only =bs4.SoupStrainer( class_=(\"post-content\", \"post-title\", \"post-header\" )))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f53f226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f71841bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b736680c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas'])\n",
      "ID: 910181a3-dee0-44c8-af8f-d3b934ecc9bb\n",
      "Text: LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool con\n",
      "Meta: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "==================================================\n",
      "ID: dc146721-f5f7-418e-82d7-83f8601ea8ff\n",
      "Text: Memory\n",
      "\n",
      "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
      "Long-term memory: This provides the agent with th\n",
      "Meta: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "==================================================\n",
      "ID: b767d5e0-ca36-42b0-9fe5-535ba6928c0e\n",
      "Text: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a s\n",
      "Meta: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "==================================================\n",
      "ID: 3ca9502c-2cbc-4d1b-894f-b8bbc8059499\n",
      "Text: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outl\n",
      "Meta: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "==================================================\n",
      "ID: 8adf90ce-36f7-41a2-a2a8-5bf9d166bdda\n",
      "Text: Self-Reflection#\n",
      "Self-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in r\n",
      "Meta: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "out = vectorstore._collection.get(include=[\"documents\", \"metadatas\"])\n",
    "print(out.keys())\n",
    "\n",
    "for i in range(min(5,len(out['ids']))):\n",
    "    print(\"ID:\", out[\"ids\"][i])\n",
    "    print(\"Text:\", out[\"documents\"][i][:200])\n",
    "    print(\"Meta:\", out[\"metadatas\"][i])\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d96de6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore._collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6e4d1167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Use the context to answer the question.\\n\\nContext:\\n{context}\\n\\nQuestion: {question}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Use the context to answer the question.\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\"\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1c586aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Task decomposition is a technique used to break down complex tasks into smaller and simpler steps, making them more manageable. It involves transforming big tasks into multiple manageable tasks and shedding light on the model's thinking process. This can be done through prompting techniques like Chain of Thought (CoT) or Tree of Thoughts, as well as using task-specific instructions or relying on external classical planners.\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name = 'gpt-3.5-turbo',temperature=0)\n",
    "def format_docs(docs):\n",
    "    return '\\n\\n'.join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e75c9966",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Do you think Amir is going to become very rich by the age 40 years old?\"\n",
    "document = 'Amir will is going to gain networth of 200 million by the time he is 40 years ols. His income will get to at least 1 million per month.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4b7a5a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "len(encoding.encode(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fe3f3dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embd = OpenAIEmbeddings()\n",
    "query_result = embd.embed_query(question) \n",
    "document_result = embd.embed_documents([document])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "09b17c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(vec1,vec2):\n",
    "    dot_product = np.dot(vec1,vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "63e59883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9074394698045489)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(query_result,document_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7d507ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\" Answer the question based on the following contex:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt)\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "20e1d8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition in a LLM-powered autonomous agent system involves breaking down complex tasks into smaller, more manageable subgoals. This process enables the agent to efficiently handle intricate tasks by dividing them into simpler steps. Task decomposition can be achieved through various methods such as prompting the LLM with specific instructions, using task-specific prompts, or incorporating human inputs. Additionally, external classical planners can be utilized for long-horizon planning, where the LLM translates the problem into a planning domain definition language, requests a classical planner to generate a plan, and then translates the plan back into natural language. This approach outsources the planning step to an external tool, which is common in certain robotic setups.'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm | StrOutputParser()\n",
    "chain.invoke({'context':docs, 'question':'What is Task Decomposition'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "65b27a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a technique used by agents to break down complex tasks into smaller and simpler steps. This process allows the agent to better plan and execute the task by dividing it into more manageable components. Techniques like Chain of Thought and Tree of Thoughts are examples of methods that help in task decomposition by breaking down big tasks into multiple smaller tasks or exploring multiple reasoning possibilities at each step.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain = (\n",
    "    {'context': retriever, 'question': RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke('What is Task Decomposition')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36efea2e",
   "metadata": {},
   "source": [
    "Multi Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "110104ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "generate_queries = (\n",
    "      prompt\n",
    "    | ChatOpenAI(model='gpt-3.5-turbo',temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x:x.split('\\n'))\n",
    ")\n",
    "\n",
    "def get_unique_union(documents):\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "    \n",
    "\n",
    "# Testing\n",
    "\n",
    "# queries = generate_queries.invoke(\"what is the Decomposition in LLM\")\n",
    "# batches = [retriever.get_relevant_documents(q) for q in queries]\n",
    "# unique_docs = get_unique_union(batches)\n",
    "\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retriever_chain = generate_queries| retriever.map() | get_unique_union\n",
    "docs = retriever_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "65fdd89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents involves breaking down complex tasks into smaller and simpler steps using techniques such as Chain of Thought (CoT) and Tree of Thoughts. This process helps the agent to utilize more test-time computation and enhance model performance on complex tasks by transforming big tasks into multiple manageable tasks. Additionally, task decomposition for LLM agents can also be achieved through simple prompting, task-specific instructions, or by relying on an external classical planner for long-horizon planning.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "final_rag_chain = ( {'context': retriever_chain,\n",
    "                     'question': itemgetter('question')}\n",
    "                   |prompt\n",
    "                   |llm|StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({'question':question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a549d1",
   "metadata": {},
   "source": [
    "RAG Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cf8d5dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents involves breaking down complex tasks into smaller and simpler steps using techniques such as prompting, task-specific instructions, and human inputs. Additionally, there is an approach called LLM+P that involves outsourcing the planning step to an external classical planner, which utilizes the Planning Domain Definition Language (PDDL) to describe the planning problem and generate a plan that is then translated back into natural language.'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion\n",
    "    |ChatOpenAI(model = 'gpt-3.5-turbo',temperature=0)\n",
    "    |StrOutputParser()\n",
    "    |(lambda x: x.split('\\n'))\n",
    "    \n",
    ")\n",
    "\n",
    "# Testing\n",
    "# results = generate_queries.invoke({'question':\"what is decomposition in LLM\"})\n",
    "\n",
    "def reciprocal_rank_fusion(results:list[list],k=60):\n",
    "\n",
    "    fused_score ={}\n",
    "\n",
    "    for docs in results:\n",
    "        for rank,doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            fused_score[doc_str] = fused_score.get(doc_str,0)+1 /(rank+k)\n",
    "\n",
    "    reranked_items =[(loads(doc),score) for doc ,score in sorted(fused_score.items(), key=lambda x:x[1],reverse=True)]\n",
    "    return reranked_items\n",
    "\n",
    "retrieval_chain = (generate_queries\n",
    "                   |retriever.map()\n",
    "                   |reciprocal_rank_fusion\n",
    ")\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "docs = retrieval_chain.invoke({'question':question})\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question:{question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "fusion_chain = ( {'context':retrieval_chain, 'question':RunnablePassthrough()}\n",
    "                |prompt\n",
    "                |llm\n",
    "                |StrOutputParser()\n",
    "\n",
    ")\n",
    "fusion_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb472bc",
   "metadata": {},
   "source": [
    "Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ac7af1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposition\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries_decomposition =(\n",
    "    prompt_decomposition\n",
    "    |llm\n",
    "    |StrOutputParser()\n",
    "    |(lambda x:x.split('\\n'))\n",
    ")\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions = generate_queries_decomposition.invoke(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11887e21",
   "metadata": {},
   "source": [
    "Answer recursively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4fe62571",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "here is the question you need to answer:\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fed5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "\n",
    "for q in questions:\n",
    "    rag_chain =({\n",
    "        'context': itemgetter('question') |retriever,\n",
    "        'question': itemgetter('question'),\n",
    "        'q_a_pairs': itemgetter('q_a_pairs')}\n",
    "        |decomposition_prompt\n",
    "        |llm\n",
    "        |StrOutputParser()\n",
    "    )\n",
    "    answer = rag_chain.invoke({'question':q,'q_a_pairs':q_a_pairs})\n",
    "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q,answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
