{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07224099",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3699: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import bs4\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import uuid\n",
    "from typing import Literal\n",
    "from operator import itemgetter\n",
    "from dotenv import load_dotenv\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough,RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate, PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain.load import loads, dumps\n",
    "from langchain.storage import InMemoryByteStore\n",
    "\n",
    "load_dotenv()\n",
    "langchain_api = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65029e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(parse_only =bs4.SoupStrainer( class_=(\"post-content\", \"post-title\", \"post-header\" )))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f53f226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs= loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f71841bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b736680c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ids', 'embeddings', 'documents', 'uris', 'included', 'data', 'metadatas'])\n",
      "ID: 62c83c79-d890-4c6a-9b6a-c12c1b825027\n",
      "Text: LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool con\n",
      "Meta: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "==================================================\n",
      "ID: 3c354a7a-c87b-4f22-ba5c-4dba87f004ca\n",
      "Text: Memory\n",
      "\n",
      "Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\n",
      "Long-term memory: This provides the agent with th\n",
      "Meta: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "==================================================\n",
      "ID: 5d29c4bf-6866-4a16-8b5c-e3ee7f87920f\n",
      "Text: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a s\n",
      "Meta: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "==================================================\n",
      "ID: aabce8a6-4c0c-47aa-a114-49e416fc887d\n",
      "Text: Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outl\n",
      "Meta: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "==================================================\n",
      "ID: 55189630-6f78-4e6e-abbe-a6c294f7bcfe\n",
      "Text: Self-Reflection#\n",
      "Self-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in r\n",
      "Meta: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "out = vectorstore._collection.get(include=[\"documents\", \"metadatas\"])\n",
    "print(out.keys())\n",
    "\n",
    "for i in range(min(5,len(out['ids']))):\n",
    "    print(\"ID:\", out[\"ids\"][i])\n",
    "    print(\"Text:\", out[\"documents\"][i][:200])\n",
    "    print(\"Meta:\", out[\"metadatas\"][i])\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c586aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition in a LLM-powered autonomous agent system involves breaking down complex tasks into smaller, more manageable subgoals. This process enables the agent to efficiently handle intricate tasks by dividing them into simpler steps. Task decomposition can be achieved through various methods such as prompting the LLM with specific instructions, using task-specific instructions, or incorporating human inputs. Additionally, external classical planners can be utilized for long-horizon planning in some approaches, where the LLM translates the problem into a planning domain language, requests a classical planner to generate a plan, and then translates the plan back into natural language. This outsourcing of the planning step to an external tool is common in certain robotic setups.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Use the context to answer the question.\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model_name = 'gpt-3.5-turbo',temperature=0)\n",
    "def format_docs(docs):\n",
    "    return '\\n\\n'.join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")\n",
    "\n",
    "question = \"Do you think Amir is going to become very rich by the age 40 years old?\"\n",
    "document = 'Amir will is going to gain networth of 200 million by the time he is 40 years ols. His income will get to at least 1 million per month.'\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "\n",
    "def cosine(vec1,vec2):\n",
    "    dot_product = np.dot(vec1,vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "\n",
    "\n",
    "prompt = \"\"\" Answer the question based on the following contex:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt)\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "embd = OpenAIEmbeddings()\n",
    "query_result = embd.embed_query(question) \n",
    "document_result = embd.embed_documents([document])[0]\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "chain.invoke({'context':docs, 'question':'What is Task Decomposition'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36efea2e",
   "metadata": {},
   "source": [
    "#### Multi Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "110104ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "generate_queries = (\n",
    "      prompt\n",
    "    | ChatOpenAI(model='gpt-3.5-turbo',temperature=0)\n",
    "    | StrOutputParser()\n",
    "    | (lambda x:x.split('\\n'))\n",
    ")\n",
    "\n",
    "def get_unique_union(documents):\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "    \n",
    "\n",
    "# Testing\n",
    "\n",
    "# queries = generate_queries.invoke(\"what is the Decomposition in LLM\")\n",
    "# batches = [retriever.get_relevant_documents(q) for q in queries]\n",
    "# unique_docs = get_unique_union(batches)\n",
    "\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retriever_chain = generate_queries| retriever.map() | get_unique_union\n",
    "docs = retriever_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "65fdd89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents involves breaking down complex tasks into smaller and simpler steps using techniques like Chain of Thought (CoT) and Tree of Thoughts. Additionally, LLM agents can also rely on external classical planners for long-horizon planning using the Planning Domain Definition Language (PDDL) as an intermediate interface.'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "final_rag_chain = ( {'context': retriever_chain,\n",
    "                     'question': itemgetter('question')}\n",
    "                   |prompt\n",
    "                   |llm|StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({'question':question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a549d1",
   "metadata": {},
   "source": [
    "#### RAG Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "cf8d5dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents involves breaking down complex tasks into smaller and simpler steps using techniques such as prompting, task-specific instructions, and human inputs. Additionally, a distinct approach known as LLM+P involves outsourcing the planning step to an external classical planner that utilizes the Planning Domain Definition Language (PDDL) to describe the planning problem and generate a plan that is then translated back into natural language. This allows LLM agents to effectively plan and execute tasks by leveraging external tools and domain-specific knowledge.'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion\n",
    "    |ChatOpenAI(model = 'gpt-3.5-turbo',temperature=0)\n",
    "    |StrOutputParser()\n",
    "    |(lambda x: x.split('\\n'))\n",
    "    \n",
    ")\n",
    "\n",
    "# Testing\n",
    "# results = generate_queries.invoke({'question':\"what is decomposition in LLM\"})\n",
    "\n",
    "def reciprocal_rank_fusion(results:list[list],k=60):\n",
    "\n",
    "    fused_score ={}\n",
    "\n",
    "    for docs in results:\n",
    "        for rank,doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            fused_score[doc_str] = fused_score.get(doc_str,0)+1 /(rank+k)\n",
    "\n",
    "    reranked_items =[(loads(doc),score) for doc ,score in sorted(fused_score.items(), key=lambda x:x[1],reverse=True)]\n",
    "    return reranked_items\n",
    "\n",
    "retrieval_chain = (generate_queries\n",
    "                   |retriever.map()\n",
    "                   |reciprocal_rank_fusion\n",
    ")\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "docs = retrieval_chain.invoke({'question':question})\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question:{question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "fusion_chain = ( {'context':retrieval_chain, 'question':RunnablePassthrough()}\n",
    "                |prompt\n",
    "                |llm\n",
    "                |StrOutputParser()\n",
    "\n",
    ")\n",
    "fusion_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb472bc",
   "metadata": {},
   "source": [
    "#### Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ac7af1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposition\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries_decomposition =(\n",
    "    prompt_decomposition\n",
    "    |llm\n",
    "    |StrOutputParser()\n",
    "    |(lambda x:x.split('\\n'))\n",
    ")\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions = generate_queries_decomposition.invoke(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11887e21",
   "metadata": {},
   "source": [
    "Answer recursively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4fe62571",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "here is the question you need to answer:\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "31fed5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "\n",
    "for q in questions:\n",
    "    rag_chain =({\n",
    "        'context': itemgetter('question') |retriever,\n",
    "        'question': itemgetter('question'),\n",
    "        'q_a_pairs': itemgetter('q_a_pairs')}\n",
    "        |decomposition_prompt\n",
    "        |llm\n",
    "        |StrOutputParser()\n",
    "    )\n",
    "    answer = rag_chain.invoke({'question':q,'q_a_pairs':q_a_pairs})\n",
    "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q,answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1976a34",
   "metadata": {},
   "source": [
    "Answer individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8eb464be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main components of an LLM-powered autonomous agent system include the LLM as the core controller, planning for task breakdown, subgoal decomposition for task management, reflection for self-criticism and learning, refinement for problem-solving enhancement, and memory for information storage and retrieval. These components work together to enable the autonomous agent system to function effectively, solve complex problems, and achieve autonomy in decision-making and task execution.'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"\"\"\n",
    "\n",
    "prompt_rag = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def retrieve_rag_chain(question,prompt,chain):\n",
    "    sub_questions = chain.invoke(question)\n",
    "\n",
    "    rag_results =[]\n",
    "\n",
    "    for q in sub_questions:\n",
    "        retrieved = retriever.get_relevant_documents(q)\n",
    "        answer =(prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved, \n",
    "                                                                \"question\": q})\n",
    "        rag_results.append(answer)\n",
    "        \n",
    "    return questions,sub_questions\n",
    "questions,answers = retrieve_rag_chain(question,prompt_rag,generate_queries_decomposition)\n",
    "\n",
    "\n",
    "def format_qa_paris(questions, answers):\n",
    "\n",
    "    fromat_strings=''\n",
    "\n",
    "    for i, (questions,answers) in enumerate(zip(questions,answers),start=1):\n",
    "         fromat_strings += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return fromat_strings.strip()  \n",
    "\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "\n",
    "context = format_qa_paris(questions,answers)\n",
    "\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "final_rag_chain=(\n",
    "    prompt\n",
    "    |llm\n",
    "    |StrOutputParser()\n",
    ").invoke({'context':context, 'question':question})\n",
    "final_rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e1b38c",
   "metadata": {},
   "source": [
    "#### Step Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7997b2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\",\"{input}\"),\n",
    "    (\"ai\",\"{output}\")\n",
    "])\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "     \"system\", \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",   \n",
    "    ),\n",
    "    few_shot_prompt,\n",
    "    (\"user\",\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# # Testing\n",
    "generate_queries_step_back = (prompt|llm|StrOutputParser())\n",
    "# generate_queries_step_back\n",
    "\n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "chain = (\n",
    "    {\"normal_context\": itemgetter('question')|retriever,\n",
    "          \"step_back_context\":generate_queries_step_back |retriever,\n",
    "          'question': itemgetter('question')}\n",
    "\n",
    "          |response_prompt\n",
    "          |llm|StrOutputParser()\n",
    "\n",
    "\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "360d62e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition in LLM refers to the process of breaking down complex tasks into smaller, more manageable subgoals. This allows the LLM (large language model) to efficiently handle intricate tasks by dividing them into smaller steps. There are several ways in which task decomposition can be achieved within the context of LLM:\n",
      "\n",
      "1. **LLM with Simple Prompting**: One approach to task decomposition involves using simple prompts to guide the LLM in breaking down tasks. For example, prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" can help the LLM organize the task into smaller components.\n",
      "\n",
      "2. **Task-Specific Instructions**: Another method is to provide task-specific instructions to the LLM. For instance, instructing the LLM to \"Write a story outline\" when the task involves creating a novel can help in task decomposition tailored to the specific requirements of the task.\n",
      "\n",
      "3. **Human Inputs**: Task decomposition can also be achieved through human inputs, where individuals provide guidance or input to the LLM on how to break down the task effectively.\n",
      "\n",
      "In addition to these methods, there is a distinct approach known as LLM+P, as proposed by Liu et al. in 2023. This approach involves leveraging an external classical planner for long-horizon planning. The process includes translating the problem into a Planning Domain Definition Language (PDDL), generating a PDDL plan using a classical planner, and then translating the plan back into natural language. This outsourcing of the planning step to an external tool assumes the availability of domain-specific PDDL and a suitable planner, which is common in certain robotic setups.\n",
      "\n",
      "Overall, task decomposition in LLM plays a crucial role in enabling the model to effectively tackle complex tasks by breaking them down into smaller, more manageable subgoals.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({'question': \"what is decomposition in LLM\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c40b36",
   "metadata": {},
   "source": [
    "#### HyDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "3f7287a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition for LLM agents can be done by using simple prompting like \"Steps for XYZ\", \"What are the subgoals for achieving XYZ?\", task-specific instructions, or with human inputs. Additionally, there is a distinct approach called LLM+P that involves relying on an external classical planner to do long-horizon planning using the Planning Domain Definition Language (PDDL) as an intermediate interface.\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde | ChatOpenAI(temperature=0) | StrOutputParser() \n",
    ")\n",
    "\n",
    "# Run\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_docs_for_retrieval.invoke({\"question\":question})\n",
    "\n",
    "retrieval_chain = generate_docs_for_retrieval | retriever \n",
    "retrieved_docs = retrieval_chain.invoke({\"question\":question})\n",
    "\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(final_rag_chain.invoke({\"context\":retrieved_docs,\"question\":question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5549375a",
   "metadata": {},
   "source": [
    "#### Routing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998a1f5f",
   "metadata": {},
   "source": [
    "##### Logical routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "3a9b0bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1900: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1913: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo-0125 since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'chain for python_docs'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RouteQuery(BaseModel):\n",
    "    datasource : Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user Question choose which datasource would be the most relevant for answering their question\"\n",
    "    )\n",
    "\n",
    "\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "system_prompt =  \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
    "\n",
    "Based on the programming language the question is referring to, route it to the relevant data source.\"\"\"\n",
    "\n",
    "question = \"\"\"Why doesn't the following code work:\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
    "prompt.invoke(\"french\")\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system',system_prompt),\n",
    "    ('user',\"{question}\")\n",
    "])\n",
    "\n",
    "router = (prompt | structured_llm)\n",
    "\n",
    "\n",
    "result = router.invoke({'question':question})\n",
    "\n",
    "def choose_route(result):\n",
    "    if \"python_docs\" in result.datasource.lower():\n",
    "        ### Logic here \n",
    "        return \"chain for python_docs\"\n",
    "    elif \"js_docs\" in result.datasource.lower():\n",
    "        ### Logic here \n",
    "        return \"chain for js_docs\"\n",
    "    else:\n",
    "        ### Logic here \n",
    "        return \"golang_docs\"\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route)\n",
    "full_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c46e1d0",
   "metadata": {},
   "source": [
    "##### Semantic routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "4fb061cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PHYSICS\n",
      "A black hole is a region in space where gravity is so strong that nothing, not even light, can escape from it. This happens when a star collapses under its own gravity, creating a point of infinite density called a singularity. The boundary of a black hole is called the event horizon, and once something crosses this boundary, it can never escape the black hole's gravitational pull.\n"
     ]
    }
   ],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "prompt_templates = [physics_template,math_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "def prompt_router(input):\n",
    "    query_embeddings = embeddings.embed_query(input['query'])\n",
    "    similarity = cosine_similarity([query_embeddings],prompt_embeddings)\n",
    "    most_similar  = prompt_templates[similarity.argmax()]\n",
    "    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")\n",
    "\n",
    "    return PromptTemplate.from_template(most_similar)\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(chain.invoke(\"What's a black hole\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b15f820",
   "metadata": {},
   "source": [
    "#### Multi-representation Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fde1c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\")\n",
    "docs.extend(loader.load())\n",
    "\n",
    "chain = (\n",
    "    {\"doc\": lambda x : x.page_content}\n",
    "    |PromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    |llm\n",
    "    |StrOutputParser()\n",
    ")\n",
    "\n",
    "summaries = chain.batch(docs,{'max_concurrency':5})\n",
    "\n",
    "vectorstore = Chroma(collection_name='summaries',\n",
    "                     embedding_function=OpenAIEmbeddings())\n",
    "store = InMemoryByteStore()\n",
    "id_key = 'doc_id'\n",
    "\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,   \n",
    ")\n",
    "\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "summary_docs = [\n",
    "    Document(page_content=d, metadata={id_key: doc_ids[i]}) \n",
    "    for i,d in enumerate(summaries)\n",
    "]\n",
    "\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))\n",
    "\n",
    "# Testing\n",
    "query = \"Memory in agents\"\n",
    "sub_docs = vectorstore.similarity_search(query,k=1)\n",
    "retrieved_docs = retriever.get_relevant_documents(query,n_results=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b91051",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
